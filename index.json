
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hi there!\nMy name is Haoran Hou, and I have completed a Master‚Äôs degree in Human and Biological Robotics at Imperial College London. My academic focus has been primarily on artificial intelligence, machine learning, and bioinformatics, with extensive research and practical experience.\nCurrently, I am seeking a suitable PhD position to further my research and studies in medical imaging and bioinformatics. My professional aspiration is to apply deep learning and machine learning technologies in these fields to advance health technology and improve the efficiency of disease diagnosis and treatment. In the future, I hope to contribute to the academic and research field, particularly in the development of intelligent health solutions.\n","date":1654041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1654041600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi there!\nMy name is Haoran Hou, and I have completed a Master‚Äôs degree in Human and Biological Robotics at Imperial College London. My academic focus has been primarily on artificial intelligence, machine learning, and bioinformatics, with extensive research and practical experience.","tags":null,"title":"Haoran Hou","type":"authors"},{"authors":null,"categories":null,"content":"Abstract Single-cell analysis helps to detect pathological tissue in biological images, which relies heavily on accurate 3D segmentation. However, current algorithms show unstable performance on variable data and specific imaging methods, e.g., Light-Sheet Fluorescence Microscopy (LSFM). This study presents a workflow from 4D reconstruction to 3D segmentation and single-cell analysis, with datasets of 48-hour post-fertilization (hpf) zebrafish embryonic hearts. Contributions conclude two innovative segmentation backbones: one integrates 3D U-Net with Kolmogorov-Arnold Networks (KANs), named 3D U-KANs; the other develops a 3D ResNet50 Encoder, achieving accuracy improvements of 2 and 4 percentage points, respectively, suited for LSFM especially. Precise segmentation results enable single-cell tracking and analysis, making sense for pathological assessments and promising insights to represent functional changes and regional biomechanics during cardiac development and regeneration.\nKEY WORDS: Machine Learning, 4D Reconstruction, Segmentation, Tracking, Zebrafish, Single-cell Analysis Introduction Cardiac structure and function rely on the coordination between cells [1]. Therefore, investigating the movement and mechanical properties at the single-cell level [2] is crucial for assessing normal and pathological hearts. Zebrafish has been widely used in cardiovascular research, serving as an excellent model to dissect the genetic and developmental underpinnings due to its transparency, genetic similarities and tractability [3]. Advanced imaging systems, such as Light-Sheet Fluorescence Microscopy (LSFM), further facilitate cardiovascular research. LSFM, as shown in Fig.1 (A), enables multi-scale imaging with high temporal resolution and low phototoxicity [4], making it ideal for capturing dynamic behavior in living hearts. Recent studies on LSFM demonstrate its utility in 4D (3D plus time series, 3D+T) reconstruction, providing powerful tools to quantify and analyze intricate dynamics of the living hearts [6]. Taylor et al. [7] introduced an adaptive prospective optical gating technique, which can continuously capture embryonic zebrafish heart images throughout the day. Liebling et al. [8] developed a post-acquisition synchronization technique that can reconstruct cardiac dynamics from ungated slice sequences, while Mickoleit et al. [9] demonstrated how high-resolution reconstruction can reveal the micro-architecture and functionality of zebrafish hearts.\nWith in vivo imaging advancements, numerous analytical tools are designed to explore cellular dynamics within 3D volumes. DeepCell 2.0 [10], 3DeeCellTracker [11], and Cellpose [12] are automated pipelines for cell segmentation and tracking. Frameworks like FlowNet [13] and FlowNet 2.0 [14] also provide robust tracking, specifically designed for rapid and irregular cellular movements. These methods are all comprehensive toolkits developed for dynamic analysis at the single-cell level, suited for cardiac development and function research.\nHowever, densely packed and rapidly moving cells severely challenge this single-cell analysis in 3D+T images, which relies heavily on accurate 3D segmentation. The U-Net [15] has become the cornerstone of medical segmentation since 2015, which leverages an encoder-decoder structure to capture local features and reconstruct them into precise segmentation maps. However, traditional convolution shows significant limitations [16] when facing difficulties [17], such as complex shapes of organs and tissues, highly nonlinear relationships between anatomical regions, and the noise introduced by imaging processes and sample motion, especially for modeling among 3D volumes. Additionally, preparing training data is challenging [18], especially for specific imaging conditions, and parameter optimization is often inevitable for different samples [11], even within the same optical system.\nTo address the gap and match LSFM characteristics, this study introduces two innovative segmentation backbones. The first integrates Kolmogorov-Arnold Networks (KANs) [19] into a 3D U-Net [20], forming a new architecture named 3D U-KANs. The second develops a 3D ResNet50 [21] Encoder to extract features. Both are trained on the self-constructed dataset, specifically for the LSFM imaging system.\nThe main contributions of this study are summarized as follows:\n3D U-KANs is the first network to integrate KANs into U-Net architecture for 3D medical image segmentation; 3D ResNet50 Encoder replaces the traditional downsampling and upsampling process, reducing the network complexity.\nAccuracy tests on multi-sample datasets show performance improvement with 2 percentage points for 3D U-KANs and 4 points for 3D ResNet50 Encoder, and great generalization, especially for the LSFM system. Tracking and single-cell analysis based on the segmented volumes further verify their effectiveness and application value.\nThe study implements a pipeline, integrating high-dimensional ‚Ä¶","date":1727395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727395200,"objectID":"3872c15b287c5b9c7bb446ff972dbd81","permalink":"https://example.com/project/external-project-copy-4/","publishdate":"2024-09-27T00:00:00Z","relpermalink":"/project/external-project-copy-4/","section":"project","summary":"Master Thesis Project at Imperial College London","tags":["Artificial Intelligence"],"title":"Machine learning for 3D segmentation of large datasets to detect normal and pathological hearts","type":"project"},{"authors":null,"categories":null,"content":"Abstract Multidisciplinary treatment (MDT) integrates experts across fields to deliver comprehensive, personalized treatment plans, particularly enhancing cancer care by improving diagnostic precision and treatment strategies. Its growing adoption underscores its value, forecasting wider implementation in future healthcare. This approach also analyzes the integration of multi-modal data, presenting new challenges and opportunities for AI-assisted diagnosis and treatment, with multitask models emerging as a response to evolving medical needs. In clinical practice, accurate tumor segmentation‚Äîproviding essential data on tumor size, shape, and volume‚Äîis vital for staging and assessing progression, while recurrence and prognosis assessments are crucial for tailoring treatments to patient-specific risks, thus enhancing outcomes and survival rates. Currently, there‚Äôs a gap in models capable of simultaneously handling multiple tasks, especially in esophageal cancer diagnosis and treatment, where interactions between tasks lack thorough exploration. Addressing this, our study introduces a multi-task prediction model for esophageal cancer, combining segmentation with risk recurrence and survival analysis through a dual attention mechanism. This model leverages task interactivity to enhance performance, offering significant clinical benefits in diagnosing and treating esophageal cancer.\nKEY WORDS: Esophageal Cancer, Image Segmentation, Prediction, Multi-modal Model, Multi-task Model. ‚Äå\n‚Äå ‚Äå\n","date":1724716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724716800,"objectID":"ffa25d99c319176ecdd1b6f498797f7d","permalink":"https://example.com/project/external-project-copy-6/","publishdate":"2024-08-27T00:00:00Z","relpermalink":"/project/external-project-copy-6/","section":"project","summary":"Project as a research assistant at Chinese University of Hong Kong","tags":["Artificial Intelligence"],"title":"Multimodal Model for Esophageal Cancer Diagnosis","type":"project"},{"authors":null,"categories":null,"content":"Project Specification This project utilizes advanced Microfluidic Integration Systems (MFIS) and Surface Enhanced Raman Scattering (SERS) technology, integrated into a single biochip, which enables the excitation and detection of specific biomarkers at various wavelengths, significantly enhancing detection sensitivity. Moreover, the project has developed an innovative gene sequencing technology that can directly and quickly identify and classify hundreds of pathogens and genetic mutations from DNA samples on the chip.\nThe core goal of the project is to develop an efficient gene detection biochip, combined with a large-scale data analysis platform, to achieve in-depth interpretation of genetic data. The chip is responsible for collecting and initially processing gene locus information, while complex data analysis tasks, such as gene mutation identification, gene association analysis, and disease prediction, are handled by the backend big data platform. We optimize these analysis processes using deep learning algorithms to ensure real-time processing and feedback of data transmitted from the chip. The platform not only supports general genetic data analysis but can also develop customized diagnostic and predictive models for specific diseases. This architectural design allows the chip to focus on efficient gene detection, with data processing handled by the backend platform, ensuring system flexibility and scalability.\n‚Äå\n‚Äå ‚Äå\n","date":1719446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719446400,"objectID":"3735020974bcf099bf082c258cf8ca04","permalink":"https://example.com/project/external-project-copy-10/","publishdate":"2024-06-27T00:00:00Z","relpermalink":"/project/external-project-copy-10/","section":"project","summary":"Research Project at University of Hong Kong","tags":["Artificial Intelligence"],"title":"Development of Biochips for Genetic Diagnosis","type":"project"},{"authors":["Imperial College London"],"categories":null,"content":"Please view the PDF for undergraduate transcripts.\n","date":1709769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709769600,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://example.com/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Please view the PDF for undergraduate transcripts.","tags":null,"title":"MSc in Human and Biological Robotics","type":"publication"},{"authors":["University of Oxford"],"categories":null,"content":"Please view the PDF for Certificate.\n","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"eac55319901285fd94717caa0c2c5261","permalink":"https://example.com/publication/journal-article-copy/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/journal-article-copy/","section":"publication","summary":"Please view the PDF for Certificate.","tags":null,"title":"Program about Artificial Intelligence and Machine Learning","type":"publication"},{"authors":["Tianjin University"],"categories":null,"content":"Please view the PDF for undergraduate transcripts.\n","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://example.com/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Please view the PDF for undergraduate transcripts.","tags":null,"title":"BSc in Electronic Information Engineering","type":"publication"},{"authors":null,"categories":null,"content":"ABSTRACT The purpose of human motion detection is to provide a system for self-analysis of events in the video data and to understand the behaviour of a person. This is a key function of intelligent video surveillance systems with a wide range of applications. In this paper, we investigate a human action recognition system based on two steps of human feature extraction and human action classification. Quantification and model transplantation for the human feature extraction step are performed based on an artificial intelligence chip.\nTo complete the human feature extraction, a humanoid target detector is first built based on the YOLOv5s framework and DeepSORT tracking, and a pose estimation detector is built to extract human bone joint points using the Alphapose algorithm. The extracted features are then fed into the action classification detector based on convolution of spatio-temporal maps to complete the construction of the human motion recognition system.\nDue to the problems of aerial perspective and complex background of surveillance videos, this work does not use the current mainstream dataset, but uses several high-resolution surveillance cameras from the Experimental Teaching Center of Information and Communication Engineering, School of Electrical Automation and Information Engineering, Tianjin University to create a 15-hour dataset with 9 types of actions in 12 indoor and outdoor scenes to train and evaluate the model. In this work, the feasibility of this human motion recognition system is verified by experiment, and its average accuracy can reach 95%, and it meets the real-time requirements.\nFinally, in this work, the embedded AR9341 chip is used as the model support to quantify and transplant the model to the human feature extraction phase, and the NPU module provided by the platform is used for acceleration, so that the human feature extraction system can have high accuracy, faster speed and a wider range of application scenarios.\nKEY WORDS: Object detection, Pose estimation, Action recognition, Artificial intelligence chip, Model quantization and transplantation ","date":1687824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687824000,"objectID":"0d3caca31b584449a0fa977260cf2378","permalink":"https://example.com/project/external-project-copy-3/","publishdate":"2023-06-27T00:00:00Z","relpermalink":"/project/external-project-copy-3/","section":"project","summary":"Bachelor Thesis Project at Tianjin University","tags":["Artificial Intelligence"],"title":"Human Action Recognition Module Design and Embedded AI Transplantation in Video Surveillance Systems","type":"project"},{"authors":["Haoran Hou","Zhouhao Jiang"],"categories":null,"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://example.com/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics, p. 91-101. doi:10.1145/3548608.3559175","tags":[],"title":"Design and simulation of an upper limb rehabilitation exoskeleton robot","type":"publication"},{"authors":null,"categories":null,"content":"Abstract This project aims to accelerate the upper limb rehabilitation speed for hemiplegia and disability caused by stroke and other diseases and designs an arm-assisted rehabilitation exoskeleton robot. According to the standard size of the human upper limb, the 3D model of the upper limb exoskeleton robot was designed in SolidWorks, and the DH parameters and kinematics equations of the robot were derived. By studying the arm posture of human upper limb movement, the state and data of each joint in the upper limb movement were analyzed. It was carried out in MATLAB to simulate the structure of the rehabilitation manipulator, analyze the motion trajectory planning, and verify the rationality of the robot‚Äôs joint movement. This paper combines the 3D model and the MATLAB simulation, the upper limb rehabilitation of the exoskeleton robot arm, lower arm, shoulder, elbow, and wrist joints working space, the kinematics, and the arm in various trajectory planning of posture were solved. It provides a theoretical basis and flexibility for improving the accuracy of upper limb rehabilitation.\n","date":1653609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653609600,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://example.com/project/example/","publishdate":"2022-05-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"Reseach Project at Chinese Academy of Sciences","tags":["Robotics"],"title":"Development of Robotic Arm to Assist Upper Limb Rehabilitation","type":"project"},{"authors":null,"categories":null,"content":"Abstract To enable blind people to perceive surrounding obstacles in real-time while traveling and improve the safety of blind people‚Äôs travel, traditional blind walking sticks have been modified. This article adopts the STM32f103zet6 microcontroller and uses embedded development design to add functions such as ultrasonic obstacle avoidance, GPS positioning, fall detection, fingerprint unlocking, and voice broadcasting to the traditional blind cane. This project utilizes the XFS5152CE chip to achieve voice broadcasting function, the AS608 optical fingerprint module to achieve fingerprint unlocking function, the ESP8266WiFi module to achieve interaction between mobile phones and intelligent guide canes, the ADXL345 module to detect the user‚Äôs status in real-time, and to achieve fall warning function. The ultrasonic obstacle avoidance module utilizes the principle of ultrasonic ranging. The GPS positioning module can receive and demodulate the broadcast C/A code signal of satellites, and achieve GPS positioning function by calculating the pseudo distance to each satellite and calculating parameters such as longitude and latitude. The feasibility of this scheme has been verified through experiments. Fingerprint unlocking, voice broadcasting, and GPS positioning functions can all operate normally. When the distance between the guide rod and the obstacle is less than 0.2m, or the forward acceleration in a certain direction of the guide rod is too large, the intelligent guide rod can send out a warning signal in a timely manner.\nKEY WORDS: STM32 microcontroller, intelligent blind cane, ultrasonic ranging, GPS, embedded Introduction 1.1 Project Overview 1.1.1 Introduction to the Project\nAs one of the assistive tools for mobility, guide canes are widely used among the blind community. In today‚Äôs technologically advanced world, the trend towards product intelligence is undeniable. Taking into consideration the overall environment of the pandemic and the daily mobility needs of the blind, this project is dedicated to designing an intelligent guide cane to further facilitate the mobility of the blind. By adding only a small amount of cost, the guide cane is equipped with an ultrasonic obstacle avoidance system, a GPS positioning system, a fall detection with one-click alarm feature, a display screen that can show personal health codes and payment codes, a voice broadcast for daily weather forecasts, and a fingerprint lock to protect important personal information such as health codes and payment codes.\n1.2 Project Plan 1.2.1 Background of the Project\nAccording to statistics from the Ministry of Health, there are as many as 14 million blind people in China, making it the country with the highest number of blind individuals in the world. On average, there is one blind person in every 100 individuals. Every year, about 450,000 new cases of blindness and visually impaired individuals are reported in China, highlighting an urgent need to address their mobility issues. The factors affecting the lives of the blind mainly include the following:\nFirst, the physical defects of the blind lead to many disadvantages in their lives; Second, due to mobility challenges, the blind find it difficult to interact with the outside world, lacking in entertainment and social interactions. Third, due to their physical disabilities, the blind face unequal treatment in social work and interpersonal interactions compared to sighted individuals.\nDue to visual impairments, the blind community has developed a communication mode different from sighted individuals for perceiving the world and conveying information.\nSurveys show that only 27% of blind individuals go out daily, 55% of them are unable to distinguish directions while traveling, and 34% worry about colliding with obstacles during their journeys. The survey concluded that the main reason for the low mobility of blind individuals is the lack of a comprehensive transportation system and friendly navigation devices, along with the low utilization rate of blind paths. There is a general desire among the blind for friendly mobility tools to ensure their safety, a dedicated transportation system for the blind, and the introduction of high-tech products for better navigation and assistance.\n1.2.2 Significance of the Project\nFirstly, the inability of the blind to easily venture outdoors hinders their interaction with the external world. Products such as blind navigators can facilitate their mobility, increase safety, and make their travels less tedious, thereby helping blind individuals integrate into modern intelligent life.\nSecondly, in the context of big data, artificial intelligence, and 5G, relying on advanced scientific and technological means to solve the mobility issues of the blind is of great significance. The blind hope to be treated equally by others, to not be isolated, and to participate in social life and activities just like sighted people. Hence, there is a strong desire for ‚Ä¶","date":1648339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648339200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://example.com/project/external-project/","publishdate":"2022-03-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"Innovation \u0026 Entrepreneurship Training Program for College Students","tags":["Artificial Intelligence"],"title":"Development of Intelligent Crutch Based on STM 32","type":"project"},{"authors":null,"categories":null,"content":"Project Background The intelligent vehicle, or wheeled mobile robot, integrates multiple functions such as environmental sensing, planning and decision-making, and autonomous driving into a comprehensive system. Aimed at enhancing future lifestyles with smarter solutions, this technology is applicable not only in military contexts but also in scientific research, intelligent rescue, and more. The vehicle‚Äôs mechanical structure comprises the body, wheels, speed sensors, rotational axles, equipped with a power driver and a camera module for environmental data collection.\nDesign Scheme Overview Employing a PID control algorithm and a CCD linear camera for black line guidance detection, processed by an LM393 comparator for microcontroller-based data collection and image recognition, this design enables path identification. Motor drive is facilitated by the PC33886 model, with speed measured by direct photoelectric sensors, and data displayed on an LCD screen. Four button keys are utilized for parameter settings, enhancing the user interface for onsite debugging. The project ingeniously incorporates a mix of photonic, mechanical, and wireless communication technologies.\nStructural and System Design Based on the principles of autonomous guide robot systems, the intelligent car is capable of autonomously recognizing and following predetermined paths. The main control module uses the STC89C51 microcontroller, focusing on centralized control and modular design, incorporating infrared sensor line tracking and trajectory detection modules. The mechanical structure is designed to minimize external light interference, ensuring balanced and stable navigation.\nHardware System Construction The system‚Äôs hardware includes the STM32F103 microcontroller as the main control module, along with power, motor drive, infrared obstacle avoidance, control terminal, and wireless video monitoring modules. This setup ensures reliable power supply and effective control over the vehicle‚Äôs dynamic monitoring functions.\nProgramming Challenges and Solutions The report also delves into the learning process for programming the STM32 microcontroller, including clock initialization, mobility function implementation, PID algorithm development, steering functionality, Bluetooth serial port configuration, and PID debugging. By exploring sensor detection, PWM output for motor driving, speed detection, and serial communication, the project overcame various developmental challenges.\nProject Summary Through teamwork, we successfully assembled the intelligent car and achieved the line-tracking task. Throughout the project, we solved multiple technical problems, gaining an in-depth understanding of the working principles of intelligent cars and mastering STM32 related skills. This experience not only enhanced our practical abilities but also strengthened our team collaboration spirit. This course has provided us with invaluable and enriching learning, contributing significantly to our academic and practical growth.\n","date":1640563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640563200,"objectID":"612676e86aed598af3351e4ee0545968","permalink":"https://example.com/project/external-project-copy/","publishdate":"2021-12-27T00:00:00Z","relpermalink":"/project/external-project-copy/","section":"project","summary":"Electronic system design during undergraduate","tags":["Electronic"],"title":"Develop a tracking and obstacle avoidance car based on STM32","type":"project"},{"authors":null,"categories":null,"content":"Project Overview This project develops an efficient mask detection system using Python, designed to autonomously identify whether individuals are wearing masks. Originating from the global COVID-19 pandemic in 2019, the context highlights the necessity of mask-wearing in public areas and the urgent need for automated systems to reduce the burden on manual checks.\nDevelopment Tools and Technical Approach Utilizing the Python IDE PyCharm and the image annotation tool Labelme as primary development and data processing tools, the project employs machine learning algorithms‚Äîspecifically, the MobileNet framework‚Äîfor in-depth training and optimization. Experimental methods include data preprocessing, model training, and testing validation, ensuring the system‚Äôs accuracy and practicality.\nCode Implementation and Performance Optimization The summary details the process from data set construction and model architecture establishment to the implementation of training strategies, emphasizing optimization measures taken during model training, such as dynamic learning rate adjustments and early stopping. These measures significantly enhance the model‚Äôs generalization ability and recognition accuracy.\nResults Display and Application Value Experimental outcomes demonstrate the system‚Äôs effective recognition of mask-wearing status in real-time video streams, validating the model‚Äôs effectiveness. The mask detection system not only achieves the technical objectives but also holds significant application potential in the field of public health management.\nConclusion This project showcases the powerful application capabilities of deep learning in the public health domain, particularly against the backdrop of the current global pandemic. The development of the mask detection system bears important social value and practical significance. With continuous technological advancement and optimization, the system is expected to play a larger role in a broader range of scenarios.\n","date":1640563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640563200,"objectID":"2825294ec8d0848d6b20f6bd5f58b747","permalink":"https://example.com/project/external-project-copy-8/","publishdate":"2021-12-27T00:00:00Z","relpermalink":"/project/external-project-copy-8/","section":"project","summary":"Machine learning and visual perception during undergraduate","tags":["Artificial Intelligence"],"title":"Epidemic prevention and control system for elevators","type":"project"},{"authors":["Haoran Hou","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://example.com/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Hugo Blox Builder, the website builder for Hugo","type":"post"},{"authors":null,"categories":null,"content":"Design description: The basic function of this design is to collect temperature and humidity, then collect light intensity (as the input signal part), and then display these three values on the digital tube (as the output signal part). Then set the upper limit value in the program, and the buzzer will alarm when the upper limit value is exceeded.\nIn the simulation part, a light bulb and a photoresistor are used to simulate a light sensor. The voltage across the photoresistor is obtained to represent the light intensity. Then an operational amplifier is used to make the change in light intensity and voltage within a certain range have a linear relationship. At this time, the light obtained Strong is just an analog value, which is then input to the PF0 port of ATmega64 for the next step of analog-to-digital conversion; in addition, similar to the photosensitive part, the temperature sensitive part uses a light bulb plus a thermosensitive resistor to simulate the temperature sensor; the humidity part uses pulse voltage frequency simulation Humidity, the humidity value we obtain at this time is already a digital quantity, which can be directly input into the PD6 port of ATmega64. The quantities we input will be used for subsequent display on the LM016L and for alarming when the set threshold is exceeded.\nIn addition, in the output part, we use LM016L for display, which displays digital quantities, and then switches the display and sets the alarm threshold through buttons.\nFor the working voltage part, except the buzzer alarm which uses DC +12V voltage, the rest uses the DC +5V voltage output by LM7805. Then the working voltage of ATmega64 is 4.5V to 5.5V, the working voltage of temperature and humidity sensor is 5V, the maximum working voltage of photodiode is generally 10 to 50V, the best working voltage of LM016L module is 5V, whichever of the above devices can work The voltage is 5V as the supply voltage. And due to the limitations of the device LM016L, the operating current is 2mA.\nElectrical schematic diagram: Printed board diagram: ","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"fe9725e95dab45066056cc1b7015b18b","permalink":"https://example.com/project/external-project-copy-5/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/external-project-copy-5/","section":"project","summary":"Proteus electronic system design and simulation during undergraduate","tags":["Electronic"],"title":"Temperature and humidity alarm \u0026 light intensity display","type":"project"},{"authors":null,"categories":null,"content":"I. Functionality Transmitter Side: The user inputs the mathematical expression to be calculated through buttons. The expression can include basic operations such as addition, subtraction, multiplication, and division (complex calculations can be added similarly). Then, the process of Encoding ‚Üí Modulating ‚Üí Sending is executed. After sending, the device immediately enters a waiting mode, awaiting a response from the receiver side.\nReceiver Side: The receiver side is in a waiting state from the start. Upon receiving a signal, it automatically performs demodulation, calculation, encoding, modulation, and then sends a response.\nOther Buttons: The ‚ÄúCarrier 1‚Äù and ‚ÄúCarrier 2‚Äù buttons can display the waveform of the carrier on the right side of the image. The processes of encoding, modulation, and reception will be shown on the waveform graph on the right side, displaying the time-domain waveform. The introduction button provides a simple explanation of the principles. The ‚Äústop‚Äù button is used to interrupt detection, and the ‚Äúout‚Äù button is used to exit this window.\nII. Principle and implementation Encoding Source Encoding: As shown in Figure, the wireless calculator is designed with 16 input buttons, which is the 4th power of 2.\nChannel Encoding: Why use Hamming encoding for the information bits? The answer is that Hamming encoding is an efficient linear block code capable of correcting single-bit errors. The 7,4 Hamming code adds three check bits to a group of 4-bit information bits and can correct minor errors through the calculation of syndromes or correction factors. This enhances the communication system‚Äôs resistance to interference, as detailed in the textbook on page 261.\nFrame Synchronization and Protection Codes: The purpose of frame synchronization is to determine the sampling moments for each bit. The frame synchronization code here is used to inform the receiving end when the valid information begins and to establish the starting moment for sampling. Barker codes, with their sharp autocorrelation peak, facilitate identification and minimize the likelihood of false synchronization. The receiver‚Äôs identification method is also simple, hence the choice of Barker codes as the true synchronization codes.\nGiven the variable length of the information bits transmitted in this system, the receiver cannot determine when to end sampling, so a Barker code is also added at the end to accurately extract the middle information bits. The role of the protection code is to\nprovide energy variation for the receiver to detect energy changes, and to prevent the delay in calculation from omitting useful information bits, offering the computer a buffer time.\nModulation The modulation method is 2FSK.\nFirst, the sampling frequency needs to be determined. It is best if the sampling frequency is consistent with the MATLAB sound function, hence 8000Hz is chosen here.\nNext, determine the bit transmission rate RB. Generally, a lower transmission rate results in higher accuracy but lower efficiency. Thus, a compromise is chosen at 100Hz. Given a sampling frequency of 8000, this means fs=100Hz and each bit is expanded to 80 samples (8000/80 = 100Hz). The normalized frequency is 0.025œÄ, and the time-domain waveform and spectrum of the transmitted bits are as follows:\nThen, determine the frequencies for carrier 1 and carrier 2. When selecting frequencies, it‚Äôs important that the main lobes of the frequency spectra for carrier 1 and carrier 2 overlap as little as possible after modulation. The width of the lobes on either side is fs, so there should be a gap of 2fs between the two carrier frequencies. Considering phase continuity, the frequencies should have an even ratio to simplify calculations and programming without the need to consider phase discontinuities. Furthermore, they should be below half the sampling frequency to satisfy the sampling theorem, i.e., below 4000Hz. Hence, 1000Hz and 2000Hz are chosen, with normalized frequencies of 0.25œÄ and 0.5œÄ respectively.\nThe modulation method involves modulating each bit (before expansion) into 80 points (corresponding to a sampling frequency of 8000, or 100Hz). If a bit is 1, then a 2000Hz sine wave repeating 20 times is used as the modulation waveform (2000/20=100Hz); if a bit is 0, then a 1000Hz sine wave repeating 10 times is used (1000/10=100Hz).\nReception and Demodulation It is important to note that the steps for demodulation are the reverse of those for modulation and correspond one-to-one.\nThe reception uses an energy detection method, which is simply a process of multiplying corresponding elements of matrices and summing them up to check if they reach a threshold value. 2FSK Non-coherent Demodulation The choice of non-coherent demodulation for 2FSK arises from the fact that coherent demodulation involves a multiplication process. At the selection stage, non-coherent demodulation was chosen because if it involves multiplication, the chance of coding errors increases ‚Ä¶","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"214ce39e09e6b24b4d98c80c1e2670a8","permalink":"https://example.com/project/external-project-copy-7/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/external-project-copy-7/","section":"project","summary":"Communication Principle during undergraduate","tags":["Electronic"],"title":"Wireless calculator design","type":"project"},{"authors":null,"categories":null,"content":"I was particularly engaged by the electronic system design courses, since I could build up my practical capability and build on the theoretical. We also the used single chip microcomputer 80C51F020 to generate a digital sine wave that could control the amplitude and frequency changes via keyboard and output stable sine wave by 12-bit DAC. From the design and simulation of the hardware circuit, to the layout and welding of the circuit board, and then to the design and debugging of the software part of the MCU, we designed a digital synthesis signal device.\nSynopsis of Digital Synthesizer Creation Objective and Innovation: At the core of this endeavor was the ambition to construct a digital synthesizer capable of generating sine waves, with enhanced user control over amplitude and frequency modulation. The project‚Äôs cornerstone was the adept use of the 80C51F020 microcontroller, showcasing an astute application of digital-to-analog conversion techniques to engineer precise and adjustable electronic signals.\nTechnical Mastery and Application Signal Range and Modulation: The project‚Äôs specification to adjust signal frequencies between 1Hz to 100Hz, alongside amplitude variations from 0.3V to 3Vp-p, demonstrates a rigorous and calculated approach to electronic signal manipulation. This precision underscores the team‚Äôs dedication to creating adaptable and user-centric electronic solutions.\nMicrocontroller Utilization: Employing the C8051F020‚Äôs 12-bit DAC was a strategic choice, spotlighting the project‚Äôs innovative use of microcontroller capabilities to ensure signal accuracy. This decision reflects a harmonious fusion of software-driven logic with the tangible dynamics of hardware execution.\nConvergence of Hardware and Software Design Comprehensive System Development: The project narrative delineates the transition from conceptual circuit design to the practical assembly and detailed microcontroller software debugging. This extensive process underscores the team‚Äôs proficiency in electronic system realization, encompassing a broad spectrum of engineering skills.\nAdvanced Signal Processing: Utilizing MATLAB for generating sine wave samples and their quantization for DAC implementation is indicative of an advanced approach to signal processing. This methodology bridges theoretical concepts with real-world technological applications, enriching the project‚Äôs technical depth.\nDesign requirements (function and indicator description) Signal frequency range: 1Hz-100Hz; step: 1Hz; accuracy: +/-5% (the above indicators are measured at room temperature) Signal amplitude: 0.3V-3Vp-p; step: 0.1V; accuracy: +/-10% (the above indicators are measured at room temperature) Load capacity: can drive 10 ohm pure group load Frequency and amplitude can be displayed through LED digital tube (can be displayed alternately through indicator lights) Effective when the keyboard is raised System hardware schematic diagram The following figure shows the simulation schematic diagram on Tina Key technical points of hardware modules DAC module This experiment uses C8051F020 built-in 12-bit voltage output DAC, which has the following main features:\nThe voltage reference of each DAC is provided at the VREFD or VREF pin.\rEach DAC has a flexible output update mechanism that allows seamless full-scale changes and supports jitter-free output updates.\rThe data and control interface between the MCU and each comparator and DAC are implemented through special registers\rThe main indicators of DAC: 12bit resolution:\nOutput setup time: 10us\rOutput voltage range: 0~VREF-1LSB\rOutput short circuit current: 15mA\rAnd we tried to use Tina simulation software to build a DAC module that can output stepped sine waves, so that we can verify the feasibility of subsequent modules on the software, especially the quality of the filtering effect.\nFilter module Generate a second-order Bessel low-pass filter through filter pro, and the parameter settings are as shown in the figure below.\nThis circuit module generates a clock signal, DACO generates a ladder signal, inputs an eighth-order ladder wave through DACO, and CLK inputs a clock signal of 100:1 (clock signal frequency: sinusoidal signal frequency) to achieve the function of the filter.\n(1) The signal frequency is adjustable. The signal frequency is controlled by changing the frequency of the ladder signal generated by DACO and changing the frequency of the signal in the CLK generation formula through program control. The highest frequency output that can be achieved is 2000Hz, and it has stable frequency output.\n(2) Signal phase modulation.\nBy selecting the DACO phase to output, digital phase modulation is achieved. The 0 phase is used when the number is 0 and the T phase is used when the number is 1.\nPower amplifier module A DC-blocking capacitor C4 is added before the power amplifier module to prevent the filter module and the power amplifier from affecting each other. , in order to prevent the base current from being ‚Ä¶","date":1574812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574812800,"objectID":"06adda1ba2daca7e9051355e569ec48b","permalink":"https://example.com/project/external-project-copy-9/","publishdate":"2019-11-27T00:00:00Z","relpermalink":"/project/external-project-copy-9/","section":"project","summary":"Electronic system design and simulation during undergraduate","tags":["Electronic"],"title":"Digital Synthesizer","type":"project"},{"authors":null,"categories":null,"content":"Hugo Blox Builder is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you‚Äôll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python\rimport pandas as pd\rdata = pd.read_csv(\u0026#34;data.csv\u0026#34;)\rdata.head()\r```\rrenders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;}\r- Hugo Modules\r- wowchemy\r- blox-plugins-netlify\r- blox-plugins-netlify-cms\r- blox-plugins-reveal\r```\rrenders as\n- Hugo Modules\r- wowchemy\r- blox-plugins-netlify\r- blox-plugins-netlify-cms\r- blox-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap\r- Mindmaps\r- Links\r- [Wowchemy Docs](https://docs.hugoblox.com/)\r- [Discord Community](https://discord.gg/z8wNYzb)\r- [GitHub](https://github.com/HugoBlox/hugo-blox-builder)\r- Features\r- Markdown formatting\r- **inline** ~~text~~ *styles*\r- multiline\rtext\r- `inline code`\r-\r```js\rconsole.log(\u0026#39;hello\u0026#39;);\rconsole.log(\u0026#39;code block\u0026#39;);\r```\r- Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$\r```\rrenders as\n- Mindmaps\r- Links\r- [Wowchemy Docs](https://docs.hugoblox.com/)\r- [Discord Community](https://discord.gg/z8wNYzb)\r- [GitHub](https://github.com/HugoBlox/hugo-blox-builder)\r- Features\r- Markdown formatting\r- **inline** ~~text~~ *styles*\r- multiline\rtext\r- `inline code`\r-\r```js\rconsole.log(\u0026#39;hello\u0026#39;);\rconsole.log(\u0026#39;code block\u0026#39;);\r```\r- Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$\rExample inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\rf(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\\r1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}\r$$\rDiagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid\rgraph TD\rA[Hard] --\u0026gt;|Text| B(Round)\rB --\u0026gt; C{Decision}\rC --\u0026gt;|One| D[Result 1]\rC --\u0026gt;|Two| E[Result 2]\r```\rrenders as\ngraph TD\rA[Hard] --\u0026gt;|Text| B(Round)\rB --\u0026gt; C{Decision}\rC --\u0026gt;|One| D[Result 1]\rC --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid\rsequenceDiagram\rAlice-\u0026gt;\u0026gt;John: Hello John, how are you?\rloop Healthcheck\rJohn-\u0026gt;\u0026gt;John: Fight against hypochondria\rend\rNote right of John: Rational thoughts!\rJohn--\u0026gt;\u0026gt;Alice: Great!\rJohn-\u0026gt;\u0026gt;Bob: How about you?\rBob--\u0026gt;\u0026gt;John: Jolly good!\r```\rrenders as\nsequenceDiagram\rAlice-\u0026gt;\u0026gt;John: Hello John, how are you?\rloop Healthcheck\rJohn-\u0026gt;\u0026gt;John: Fight against hypochondria\rend\rNote right of John: Rational thoughts!\rJohn--\u0026gt;\u0026gt;Alice: Great!\rJohn-\u0026gt;\u0026gt;Bob: How about you?\rBob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid\rgantt\rsection Section\rCompleted :done, des1, 2014-01-06,2014-01-08\rActive :active, des2, 2014-01-07, 3d\rParallel 1 : des3, after des1, 1d\rParallel 2 : des4, after des1, 1d\rParallel 3 : des5, after des3, 1d\rParallel 4 : des6, after des4, 1d\r```\rrenders as\ngantt\rsection ‚Ä¶","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://example.com/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Hugo Blox Builder is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"\u0026lt;!DOCTYPE html\u0026gt; Haoran H ou Email : haoran . hou 23@imperial.ac.uk | houhaoran0826@163.com EDUCATION Imperial College London 09 /20 23 - 09/2024 ÔÉò Major : Human and Biological Robotics (MSc, Merit) ÔÉò Main Courses : Systems Physiology | Brain Machine Interfaces | Image Processing | Reinforcement Learning | Statistics and Data Analysis | Robotics | Bioinspired Robots | Application Specific Integrated Circuits | Human Neuromechanical Control and Learning Un iversity of Oxford 01/202 3-03/2023 ÔÉò Major : Artificial Intelligence and Machine Learning ÔÉò A Joint School Programme with Tianjin University Tianjin University 09/2019-06/2023 ÔÉò Major : Electronic Information Engineering (B.Eng, GPA: 87. 96 /100 , Top 20%) ÔÉò Main Courses : Machine Learning and Visual Perception | Deep Learning | Statistics and Data Analysis | Pattern Recognition and Brain-Insipired Intelligence | Digital Image Processing | A utomatic C ontrol | Electronic Circuit | Electronic System Design | Wireless Sensor Network Technology RESEARCH PROJECTS Machine Learning for 3D S egmentation of L arge D atasets to D etect N ormal and P athological H earts Imperial College London 0 1/2024-09/2024 ÔÉò Data source: 48 hpf zebrafish heart 2D slices (20 GB). ÔÉò Imaging method : Light-Sheet Fluorescence Microscope . ÔÉò 4D Reconstruction : heartbeat cycle detection, spatial alignment, temporal alignment, and resampling. ÔÉò 3D Segmentation: propose 2 backbones (3D U-KANs and 3D ResNet50 Encoder) innovately. ÔÉò Tracking and single cell analysis using segmentation results. Multimodal Model for Esophageal Cancer Diagnosis Chinese University of Hong Kong 06/2024-09/2024 ÔÉò Datasets: CT images and unstructured text of esophageal tissues. ÔÉò Develop a multimodal model for segmentation and diagnosis of esophageal cancer lesions. ÔÉò Use NLP to embed esophageal cancer text information into the segmentation network through CLIP model . ÔÉò Transfer the model to use in lung cancer diagnosis. Development of Biochips for Genetic Diagnosis University of Hong Kong 02/2024-05/2024 ÔÉò FPGA-based processors: accelerate bioinformatics computing. ÔÉò Real-time CRISPR technology: simultaneous detection of multiple genes. ÔÉò Microfluidics technology: rapid identification and editing of target genes. ÔÉò Machine learning algorithms: identify and classify complex image data from biochips. Haoran H ou Email : haoran . hou 23@imperial.ac.uk | houhaoran0826@163.com Development of Robotic Arm to Assist Upper Limb Rehabilitation Chinese Academy of Sciences 05/2022-11/2022 ÔÉò Paper: Design and simulation of an upper limb rehabilitation exoskeleton robot ( doi:10.1145/3548608.3559175 ) ÔÉò Analyze dynamics and kinematics to design a reasonable exoskeleton robot structure . ÔÉò Analyze the structure and joint motion of the human arm to plan the trajectory of the robot arm . ÔÉò Use impedance control to optimize the control system. Development of Intelligent Crutch Based on STM 32 Innovation \u0026amp; Entrepreneurship Training Program for College Students 05/2022-06/2023 ÔÉò Develop a smart crutch using artificial intelligence and embedded systems . ÔÉò Add functions such as GPS, ultrasonic obstacle avoidance and fall detection to traditional crutches. ÔÉò Provid e real-time communication, seamless integration , data exchange, instant access to user information. INTERNSHIP Huatai Securities Position: Wealth Center Intern 08/2023-09/2023 ÔÉò Analyz e stock market trends, client investment portfolios, etc. ÔÉò Us e data models to predict market trends, and turn it into useful investment strategies. China Pacific Life Insurance Co., Ltd. Position: Account Manager Intern 07/2023-08/2023 ÔÉò Collect e , analyz e , and interpret e customer data. ÔÉò Craft superior marketing strategies and service programs using data. CERTIFICATE (COURSERA) ÔÉò DevOps, DataOps, MLOps Duke University ÔÉò Database Design and Basic SQL in PostgreSQL University of Michigan ÔÉò Recommendation Systems on Google Cloud Google Cloud ÔÉò ETL and Data Pipelines with Shell, Airflow and Kafka IBM ÔÉò Generative AI Language Modeling with Transformers IBM ÔÉò Building Interactive User Interfaces Using React Library NIIT ÔÉò Probabilistic Deep Learning with TensorFlow2 Imperial College London SKILLS ÔÉò P r ogramming Languages \u0026amp; Tools: C++, Python, MATLAB, Solidworks , Spark, Cadence virtuoso ÔÉò Da ta Science : SQL , PostgreSQL , ETL pipeline development , Azure DevOps , Google Cloud ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8dd2c01f93834b93a76bf894a31ac552","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"\u003c!DOCTYPE html\u003e Haoran H ou Email : haoran . hou 23@imperial.ac.uk | houhaoran0826@163.com EDUCATION Imperial College London 09 /20 23 - 09/2024 ÔÉò Major : Human and Biological Robotics (MSc, Merit) ÔÉò Main Courses : Systems Physiology | Brain Machine Interfaces | Image Processing | Reinforcement Learning | Statistics and Data Analysis | Robotics | Bioinspired Robots | Application Specific Integrated Circuits | Human Neuromechanical Control and Learning Un iversity of Oxford 01/202 3-03/2023 ÔÉò Major : Artificial Intelligence and Machine Learning ÔÉò A Joint School Programme with Tianjin University Tianjin University 09/2019-06/2023 ÔÉò Major : Electronic Information Engineering (B.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/","section":"event","summary":"","tags":null,"title":"","type":"event"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://example.com/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://example.com/terms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/terms/","section":"","summary":"","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Organize your notebooks Place the notebooks that you would like to publish in a notebooks folder at the root of your website.\nOrganize your notebooks The notebooks will be published to the folder you specify above. In this case, they will be published to your content/post/ folder.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"94fa5e486d3bf3e0941e2ff6e7126c06","permalink":"https://example.com/post/blog-with-jupyter/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/blog-with-jupyter/","section":"post","summary":"Relevant courses and grades","tags":null,"title":"Master - Imperial College London - Human and Biological robotics","type":"post"}]